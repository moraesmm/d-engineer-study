{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyIceberg - Apache Iceberg com Python\n",
    "\n",
    "Este notebook cobre:\n",
    "- O que é Apache Iceberg\n",
    "- Configuração do catálogo\n",
    "- Criação e gerenciamento de tabelas\n",
    "- Time Travel e Snapshots\n",
    "- Schema Evolution\n",
    "- Partitioning e Hidden Partitioning\n",
    "- Integração com Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é Apache Iceberg?\n",
    "\n",
    "Apache Iceberg é um **formato de tabela aberto** para datasets analíticos grandes. \n",
    "\n",
    "### Vantagens:\n",
    "- **ACID Transactions**: Operações atômicas\n",
    "- **Time Travel**: Acesso a versões históricas\n",
    "- **Schema Evolution**: Alterar schema sem reescrever dados\n",
    "- **Hidden Partitioning**: Particionamento transparente\n",
    "- **Otimizações**: Compaction, file pruning, etc.\n",
    "\n",
    "### Comparação com Delta Lake:\n",
    "| Feature | Iceberg | Delta Lake |\n",
    "|---------|---------|------------|\n",
    "| Open Source | Apache | Databricks |\n",
    "| Engines | Multi-engine | Spark focused |\n",
    "| Hidden Partitioning | Sim | Não |\n",
    "| Partition Evolution | Sim | Limitado |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalação e Setup\n",
    "\n",
    "```bash\n",
    "pip install pyiceberg\n",
    "pip install pyiceberg[s3,glue]  # Com extras para AWS\n",
    "pip install pyiceberg[pyarrow]  # Com PyArrow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports básicos\n",
    "from pyiceberg.catalog import load_catalog\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import (\n",
    "    NestedField, StringType, LongType, DoubleType, \n",
    "    TimestampType, DateType, BooleanType\n",
    ")\n",
    "from pyiceberg.partitioning import PartitionSpec, PartitionField\n",
    "from pyiceberg.transforms import YearTransform, MonthTransform, DayTransform, IdentityTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configurando Catálogos\n",
    "\n",
    "PyIceberg suporta vários tipos de catálogo:\n",
    "- **SQL Catalog**: SQLite, PostgreSQL\n",
    "- **Glue Catalog**: AWS Glue\n",
    "- **Hive Catalog**: Hive Metastore\n",
    "- **REST Catalog**: REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catálogo SQLite (local, para desenvolvimento)\n",
    "catalog_sqlite = load_catalog(\n",
    "    \"sqlite_catalog\",\n",
    "    **{\n",
    "        \"type\": \"sql\",\n",
    "        \"uri\": \"sqlite:///./data/iceberg_catalog.db\",\n",
    "        \"warehouse\": \"./data/iceberg_warehouse\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catálogo AWS Glue (produção)\n",
    "# catalog_glue = load_catalog(\n",
    "#     \"glue_catalog\",\n",
    "#     **{\n",
    "#         \"type\": \"glue\",\n",
    "#         \"warehouse\": \"s3://my-bucket/iceberg-warehouse\",\n",
    "#         \"region_name\": \"us-east-1\"\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catálogo REST (Tabular, Nessie, etc.)\n",
    "# catalog_rest = load_catalog(\n",
    "#     \"rest_catalog\",\n",
    "#     **{\n",
    "#         \"type\": \"rest\",\n",
    "#         \"uri\": \"http://localhost:8181\",\n",
    "#         \"warehouse\": \"s3://my-bucket/warehouse\"\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Definindo Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema de exemplo - Vendas\n",
    "schema = Schema(\n",
    "    NestedField(field_id=1, name=\"id\", field_type=LongType(), required=True),\n",
    "    NestedField(field_id=2, name=\"produto\", field_type=StringType(), required=True),\n",
    "    NestedField(field_id=3, name=\"categoria\", field_type=StringType(), required=False),\n",
    "    NestedField(field_id=4, name=\"quantidade\", field_type=LongType(), required=True),\n",
    "    NestedField(field_id=5, name=\"preco_unitario\", field_type=DoubleType(), required=True),\n",
    "    NestedField(field_id=6, name=\"data_venda\", field_type=DateType(), required=True),\n",
    "    NestedField(field_id=7, name=\"regiao\", field_type=StringType(), required=False),\n",
    ")\n",
    "\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Partitioning\n",
    "\n",
    "### Hidden Partitioning\n",
    "Iceberg suporta **hidden partitioning** - usuários não precisam saber como os dados são particionados.\n",
    "\n",
    "### Transforms disponíveis:\n",
    "- `identity`: valor exato\n",
    "- `year`, `month`, `day`, `hour`: para timestamps/dates\n",
    "- `bucket(n)`: hash em n buckets\n",
    "- `truncate(n)`: primeiros n caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition Spec com transforms\n",
    "partition_spec = PartitionSpec(\n",
    "    PartitionField(\n",
    "        source_id=6,  # data_venda\n",
    "        field_id=1000,\n",
    "        transform=MonthTransform(),\n",
    "        name=\"data_venda_month\"\n",
    "    ),\n",
    "    PartitionField(\n",
    "        source_id=7,  # regiao\n",
    "        field_id=1001,\n",
    "        transform=IdentityTransform(),\n",
    "        name=\"regiao\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(partition_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Criando e Gerenciando Tabelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar namespace (database)\n",
    "try:\n",
    "    catalog_sqlite.create_namespace(\"vendas_db\")\n",
    "except Exception as e:\n",
    "    print(f\"Namespace já existe ou erro: {e}\")\n",
    "\n",
    "# Listar namespaces\n",
    "print(\"Namespaces:\", catalog_sqlite.list_namespaces())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar tabela\n",
    "table_name = \"vendas_db.vendas\"\n",
    "\n",
    "try:\n",
    "    table = catalog_sqlite.create_table(\n",
    "        identifier=table_name,\n",
    "        schema=schema,\n",
    "        partition_spec=partition_spec,\n",
    "        properties={\n",
    "            \"write.format.default\": \"parquet\",\n",
    "            \"write.parquet.compression-codec\": \"snappy\"\n",
    "        }\n",
    "    )\n",
    "    print(f\"Tabela {table_name} criada!\")\n",
    "except Exception as e:\n",
    "    print(f\"Tabela já existe: {e}\")\n",
    "    table = catalog_sqlite.load_table(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar tabelas\n",
    "print(\"Tabelas:\", catalog_sqlite.list_tables(\"vendas_db\"))\n",
    "\n",
    "# Carregar tabela existente\n",
    "table = catalog_sqlite.load_table(table_name)\n",
    "print(f\"\\nSchema: {table.schema()}\")\n",
    "print(f\"\\nPartition Spec: {table.spec()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inserindo Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from datetime import date\n",
    "\n",
    "# Criar dados com PyArrow\n",
    "data = pa.table({\n",
    "    \"id\": [1, 2, 3, 4, 5],\n",
    "    \"produto\": [\"Notebook\", \"Mouse\", \"Teclado\", \"Monitor\", \"Webcam\"],\n",
    "    \"categoria\": [\"Computadores\", \"Periféricos\", \"Periféricos\", \"Computadores\", \"Periféricos\"],\n",
    "    \"quantidade\": [10, 50, 30, 15, 25],\n",
    "    \"preco_unitario\": [3500.0, 150.0, 300.0, 1200.0, 400.0],\n",
    "    \"data_venda\": [date(2024, 1, 15), date(2024, 1, 20), date(2024, 2, 10), date(2024, 2, 15), date(2024, 3, 1)],\n",
    "    \"regiao\": [\"Sul\", \"Sudeste\", \"Sul\", \"Norte\", \"Sudeste\"]\n",
    "})\n",
    "\n",
    "# Append dados\n",
    "table.append(data)\n",
    "print(\"Dados inseridos!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite (substitui todos os dados)\n",
    "# table.overwrite(data)\n",
    "\n",
    "# Delete (baseado em expressão)\n",
    "# from pyiceberg.expressions import EqualTo\n",
    "# table.delete(EqualTo(\"regiao\", \"Norte\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Lendo Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan completo\n",
    "scan = table.scan()\n",
    "df = scan.to_arrow()\n",
    "print(df.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan com seleção de colunas\n",
    "scan = table.scan(selected_fields=(\"produto\", \"quantidade\", \"preco_unitario\"))\n",
    "df = scan.to_arrow()\n",
    "print(df.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.expressions import GreaterThan, EqualTo, And\n",
    "\n",
    "# Scan com filtros (row filter)\n",
    "scan = table.scan(\n",
    "    row_filter=And(\n",
    "        GreaterThan(\"quantidade\", 20),\n",
    "        EqualTo(\"regiao\", \"Sul\")\n",
    "    )\n",
    ")\n",
    "df = scan.to_arrow()\n",
    "print(df.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Time Travel e Snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar snapshots\n",
    "for snapshot in table.snapshots():\n",
    "    print(f\"Snapshot ID: {snapshot.snapshot_id}\")\n",
    "    print(f\"  Timestamp: {snapshot.timestamp_ms}\")\n",
    "    print(f\"  Operation: {snapshot.summary}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserir mais dados para criar novo snapshot\n",
    "novos_dados = pa.table({\n",
    "    \"id\": [6, 7],\n",
    "    \"produto\": [\"Headset\", \"SSD\"],\n",
    "    \"categoria\": [\"Periféricos\", \"Armazenamento\"],\n",
    "    \"quantidade\": [40, 20],\n",
    "    \"preco_unitario\": [250.0, 500.0],\n",
    "    \"data_venda\": [date(2024, 3, 15), date(2024, 3, 20)],\n",
    "    \"regiao\": [\"Sul\", \"Sudeste\"]\n",
    "})\n",
    "\n",
    "table.append(novos_dados)\n",
    "print(\"Novos dados inseridos!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time travel - ler snapshot específico\n",
    "snapshots = list(table.snapshots())\n",
    "if len(snapshots) > 1:\n",
    "    old_snapshot_id = snapshots[0].snapshot_id\n",
    "    \n",
    "    # Scan de snapshot antigo\n",
    "    scan = table.scan(snapshot_id=old_snapshot_id)\n",
    "    df_old = scan.to_arrow()\n",
    "    print(f\"Dados do snapshot {old_snapshot_id}:\")\n",
    "    print(df_old.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados atuais\n",
    "scan = table.scan()\n",
    "df_current = scan.to_arrow()\n",
    "print(\"Dados atuais:\")\n",
    "print(df_current.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Schema Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar coluna\n",
    "with table.update_schema() as update:\n",
    "    update.add_column(\"desconto\", DoubleType(), doc=\"Desconto aplicado\")\n",
    "\n",
    "print(\"Nova coluna adicionada!\")\n",
    "print(table.schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomear coluna\n",
    "# with table.update_schema() as update:\n",
    "#     update.rename_column(\"preco_unitario\", \"valor_unitario\")\n",
    "\n",
    "# Alterar tipo (upcasting permitido)\n",
    "# with table.update_schema() as update:\n",
    "#     update.update_column(\"quantidade\", DoubleType())\n",
    "\n",
    "# Tornar nullable\n",
    "# with table.update_schema() as update:\n",
    "#     update.make_column_optional(\"quantidade\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Partition Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver particionamento atual\n",
    "print(\"Particionamento atual:\")\n",
    "print(table.spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evoluir particionamento (novos dados usarão novo spec)\n",
    "# with table.update_spec() as update:\n",
    "#     update.add_field(\n",
    "#         source_column_name=\"categoria\",\n",
    "#         transform=IdentityTransform(),\n",
    "#         partition_field_name=\"categoria\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Integração com Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração Spark com Iceberg\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"IcebergSpark\") \\\n",
    "#     .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.0\") \\\n",
    "#     .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "#     .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "#     .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "#     .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "#     .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "#     .config(\"spark.sql.catalog.local.warehouse\", \"./iceberg-warehouse\") \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operações SQL com Iceberg no Spark\n",
    "\n",
    "# Criar tabela\n",
    "# spark.sql(\"\"\"\n",
    "#     CREATE TABLE local.db.vendas (\n",
    "#         id BIGINT,\n",
    "#         produto STRING,\n",
    "#         valor DOUBLE,\n",
    "#         data_venda DATE\n",
    "#     )\n",
    "#     USING iceberg\n",
    "#     PARTITIONED BY (months(data_venda))\n",
    "# \"\"\")\n",
    "\n",
    "# Inserir dados\n",
    "# spark.sql(\"\"\"\n",
    "#     INSERT INTO local.db.vendas VALUES\n",
    "#     (1, 'Notebook', 3500.0, DATE '2024-01-15'),\n",
    "#     (2, 'Mouse', 150.0, DATE '2024-02-20')\n",
    "# \"\"\")\n",
    "\n",
    "# Time travel\n",
    "# spark.sql(\"SELECT * FROM local.db.vendas VERSION AS OF 1\")\n",
    "# spark.sql(\"SELECT * FROM local.db.vendas TIMESTAMP AS OF '2024-01-01 00:00:00'\")\n",
    "\n",
    "# Histórico\n",
    "# spark.sql(\"SELECT * FROM local.db.vendas.history\")\n",
    "# spark.sql(\"SELECT * FROM local.db.vendas.snapshots\")\n",
    "# spark.sql(\"SELECT * FROM local.db.vendas.files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Manutenção de Tabelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadados da tabela\n",
    "print(f\"Location: {table.location()}\")\n",
    "print(f\"Current Snapshot: {table.current_snapshot()}\")\n",
    "print(f\"Properties: {table.properties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Via Spark SQL - Manutenção\n",
    "\n",
    "# Expire snapshots antigos\n",
    "# spark.sql(\"CALL local.system.expire_snapshots('db.vendas', TIMESTAMP '2024-01-01')\")\n",
    "\n",
    "# Remover arquivos órfãos\n",
    "# spark.sql(\"CALL local.system.remove_orphan_files('db.vendas')\")\n",
    "\n",
    "# Reescrever arquivos (compaction)\n",
    "# spark.sql(\"CALL local.system.rewrite_data_files('db.vendas')\")\n",
    "\n",
    "# Reescrever manifests\n",
    "# spark.sql(\"CALL local.system.rewrite_manifests('db.vendas')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo de Comandos PyIceberg\n",
    "\n",
    "```python\n",
    "# Catálogo\n",
    "catalog = load_catalog(\"name\", **config)\n",
    "catalog.list_namespaces()\n",
    "catalog.create_namespace(\"db\")\n",
    "catalog.list_tables(\"db\")\n",
    "\n",
    "# Tabela\n",
    "table = catalog.create_table(identifier, schema, partition_spec)\n",
    "table = catalog.load_table(\"db.tabela\")\n",
    "\n",
    "# CRUD\n",
    "table.append(pyarrow_table)\n",
    "table.overwrite(pyarrow_table)\n",
    "table.delete(expression)\n",
    "\n",
    "# Scan\n",
    "scan = table.scan(selected_fields, row_filter, snapshot_id)\n",
    "df = scan.to_arrow()\n",
    "\n",
    "# Schema Evolution\n",
    "with table.update_schema() as update:\n",
    "    update.add_column(\"col\", Type())\n",
    "    update.rename_column(\"old\", \"new\")\n",
    "\n",
    "# Snapshots\n",
    "table.snapshots()\n",
    "table.current_snapshot()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
