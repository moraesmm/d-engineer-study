{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS S3 com PySpark\n",
    "\n",
    "Este notebook cobre:\n",
    "- Configuração do Spark para S3\n",
    "- Leitura e escrita de dados no S3\n",
    "- Boas práticas e otimizações\n",
    "- Formatos de arquivo recomendados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuração do Spark para S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configuração básica para S3\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkS3\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"YOUR_ACCESS_KEY\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"YOUR_SECRET_KEY\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando IAM Role (recomendado em EMR/EC2)\n",
    "spark_iam = SparkSession.builder \\\n",
    "    .appName(\"SparkS3IAM\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \n",
    "            \"com.amazonaws.auth.InstanceProfileCredentialsProvider\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando profile do AWS CLI\n",
    "spark_profile = SparkSession.builder \\\n",
    "    .appName(\"SparkS3Profile\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \n",
    "            \"com.amazonaws.auth.profile.ProfileCredentialsProvider\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configurações de Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark otimizado para S3\n",
    "spark_optimized = SparkSession.builder \\\n",
    "    .appName(\"SparkS3Optimized\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"YOUR_ACCESS_KEY\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"YOUR_SECRET_KEY\") \\\n",
    "    \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"100\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.fast.upload.buffer\", \"bytebuffer\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.multipart.size\", \"104857600\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.block.size\", \"134217728\") \\\n",
    "    \\\n",
    "    .config(\"spark.sql.parquet.mergeSchema\", \"false\") \\\n",
    "    .config(\"spark.sql.parquet.filterPushdown\", \"true\") \\\n",
    "    .config(\"spark.hadoop.parquet.enable.summary-metadata\", \"false\") \\\n",
    "    \\\n",
    "    .config(\"spark.speculation\", \"false\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lendo Dados do S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo Parquet\n",
    "df_parquet = spark.read.parquet(\"s3a://my-bucket/data/parquet/\")\n",
    "df_parquet.show()\n",
    "\n",
    "# Lendo particionado\n",
    "df_partitioned = spark.read.parquet(\"s3a://my-bucket/data/partitioned/\")\n",
    "df_partitioned.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo CSV\n",
    "df_csv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"s3a://my-bucket/data/csv/\")\n",
    "\n",
    "# Lendo JSON\n",
    "df_json = spark.read.json(\"s3a://my-bucket/data/json/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo com glob pattern\n",
    "df_multi = spark.read.parquet(\"s3a://my-bucket/data/year=2024/month=*/\")\n",
    "\n",
    "# Lendo múltiplos paths\n",
    "paths = [\n",
    "    \"s3a://my-bucket/data/region=us/\",\n",
    "    \"s3a://my-bucket/data/region=eu/\"\n",
    "]\n",
    "df_regions = spark.read.parquet(*paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Escrevendo Dados no S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de exemplo\n",
    "data = [\n",
    "    (1, \"João\", \"Vendas\", 5000.0, \"2024-01-15\"),\n",
    "    (2, \"Maria\", \"TI\", 7500.0, \"2024-01-20\"),\n",
    "    (3, \"Pedro\", \"Vendas\", 4500.0, \"2024-02-10\"),\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"id\", \"nome\", \"departamento\", \"salario\", \"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escrevendo Parquet\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"s3a://my-bucket/output/parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escrevendo particionado\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"departamento\") \\\n",
    "    .parquet(\"s3a://my-bucket/output/partitioned/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escrevendo com compressão específica\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .parquet(\"s3a://my-bucket/output/compressed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controlando número de arquivos\n",
    "df.coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"s3a://my-bucket/output/single-file/\")\n",
    "\n",
    "# Ou usando repartition para distribuir melhor\n",
    "df.repartition(4) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"s3a://my-bucket/output/four-files/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dynamic Partition Overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar modo dinâmico\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "# Agora só sobrescreve partições presentes no DataFrame\n",
    "df_vendas = df.filter(df.departamento == \"Vendas\")\n",
    "df_vendas.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"departamento\") \\\n",
    "    .parquet(\"s3a://my-bucket/output/partitioned/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. S3 Select (Pushdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Select permite filtrar dados no S3 antes de transferir\n",
    "# Requer formato CSV ou JSON no S3\n",
    "\n",
    "# Configurar S3 Select\n",
    "df_s3select = spark.read \\\n",
    "    .format(\"s3selectCSV\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .load(\"s3a://my-bucket/data/large.csv\")\n",
    "\n",
    "# Filtro será pushdown para S3\n",
    "df_filtered = df_s3select.filter(df_s3select.salario > 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Boas Práticas S3 + Spark\n",
    "\n",
    "### Estrutura de Diretórios:\n",
    "```\n",
    "s3://bucket/\n",
    "├── raw/                    # Dados brutos\n",
    "│   └── source/\n",
    "│       └── year=2024/\n",
    "│           └── month=01/\n",
    "├── processed/              # Dados processados\n",
    "│   └── domain/\n",
    "│       └── table/\n",
    "├── curated/                # Dados curados\n",
    "└── temp/                   # Arquivos temporários\n",
    "```\n",
    "\n",
    "### Dicas:\n",
    "1. Use Parquet ou ORC (nunca CSV para analytics)\n",
    "2. Particione por colunas frequentemente filtradas\n",
    "3. Evite arquivos muito pequenos (< 128MB)\n",
    "4. Use compressão Snappy ou ZSTD\n",
    "5. Prefira s3a:// ao invés de s3://"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de pipeline completo\n",
    "def process_s3_data(input_path, output_path):\n",
    "    # Ler dados\n",
    "    df = spark.read.parquet(input_path)\n",
    "    \n",
    "    # Transformar\n",
    "    df_processed = df \\\n",
    "        .filter(df.valor > 0) \\\n",
    "        .withColumn(\"ano\", F.year(\"data\")) \\\n",
    "        .withColumn(\"mes\", F.month(\"data\"))\n",
    "    \n",
    "    # Escrever particionado\n",
    "    df_processed \\\n",
    "        .repartition(\"ano\", \"mes\") \\\n",
    "        .write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"ano\", \"mes\") \\\n",
    "        .option(\"compression\", \"snappy\") \\\n",
    "        .parquet(output_path)\n",
    "\n",
    "# process_s3_data(\n",
    "#     \"s3a://my-bucket/raw/vendas/\",\n",
    "#     \"s3a://my-bucket/processed/vendas/\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
