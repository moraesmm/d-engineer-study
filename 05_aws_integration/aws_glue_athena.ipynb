{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Glue e Athena\n",
    "\n",
    "Este notebook cobre:\n",
    "- AWS Glue Catalog\n",
    "- AWS Glue ETL Jobs\n",
    "- Athena com Python\n",
    "- Integração Spark + Glue Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. AWS Glue Catalog com Boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Cliente Glue\n",
    "glue_client = boto3.client('glue', region_name='us-east-1')\n",
    "\n",
    "# Listar databases\n",
    "response = glue_client.get_databases()\n",
    "for db in response['DatabaseList']:\n",
    "    print(f\"Database: {db['Name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar database\n",
    "glue_client.create_database(\n",
    "    DatabaseInput={\n",
    "        'Name': 'meu_database',\n",
    "        'Description': 'Database para estudos'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar tabela no Glue Catalog\n",
    "glue_client.create_table(\n",
    "    DatabaseName='meu_database',\n",
    "    TableInput={\n",
    "        'Name': 'vendas',\n",
    "        'Description': 'Tabela de vendas',\n",
    "        'StorageDescriptor': {\n",
    "            'Columns': [\n",
    "                {'Name': 'id', 'Type': 'bigint'},\n",
    "                {'Name': 'produto', 'Type': 'string'},\n",
    "                {'Name': 'valor', 'Type': 'double'},\n",
    "                {'Name': 'quantidade', 'Type': 'int'},\n",
    "            ],\n",
    "            'Location': 's3://my-bucket/data/vendas/',\n",
    "            'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',\n",
    "            'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',\n",
    "            'SerdeInfo': {\n",
    "                'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
    "            },\n",
    "            'Compressed': True\n",
    "        },\n",
    "        'PartitionKeys': [\n",
    "            {'Name': 'ano', 'Type': 'int'},\n",
    "            {'Name': 'mes', 'Type': 'int'}\n",
    "        ],\n",
    "        'TableType': 'EXTERNAL_TABLE',\n",
    "        'Parameters': {\n",
    "            'classification': 'parquet',\n",
    "            'parquet.compression': 'SNAPPY'\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar tabelas\n",
    "response = glue_client.get_tables(DatabaseName='meu_database')\n",
    "for table in response['TableList']:\n",
    "    print(f\"Tabela: {table['Name']}\")\n",
    "    print(f\"  Location: {table['StorageDescriptor']['Location']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar partição\n",
    "glue_client.create_partition(\n",
    "    DatabaseName='meu_database',\n",
    "    TableName='vendas',\n",
    "    PartitionInput={\n",
    "        'Values': ['2024', '01'],\n",
    "        'StorageDescriptor': {\n",
    "            'Columns': [\n",
    "                {'Name': 'id', 'Type': 'bigint'},\n",
    "                {'Name': 'produto', 'Type': 'string'},\n",
    "                {'Name': 'valor', 'Type': 'double'},\n",
    "                {'Name': 'quantidade', 'Type': 'int'},\n",
    "            ],\n",
    "            'Location': 's3://my-bucket/data/vendas/ano=2024/mes=01/',\n",
    "            'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',\n",
    "            'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',\n",
    "            'SerdeInfo': {\n",
    "                'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSCK Repair - Descobrir partições automaticamente\n",
    "# Isso é feito via Athena ou Spark, não diretamente no Glue\n",
    "# Use: ALTER TABLE vendas RECOVER PARTITIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AWS Glue Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar Crawler\n",
    "glue_client.create_crawler(\n",
    "    Name='vendas-crawler',\n",
    "    Role='arn:aws:iam::123456789:role/GlueServiceRole',\n",
    "    DatabaseName='meu_database',\n",
    "    Targets={\n",
    "        'S3Targets': [\n",
    "            {\n",
    "                'Path': 's3://my-bucket/data/vendas/',\n",
    "                'Exclusions': ['*.tmp', '_SUCCESS']\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    SchemaChangePolicy={\n",
    "        'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
    "        'DeleteBehavior': 'LOG'\n",
    "    },\n",
    "    Configuration='''{\n",
    "        \"Version\": 1.0,\n",
    "        \"CrawlerOutput\": {\n",
    "            \"Partitions\": { \"AddOrUpdateBehavior\": \"InheritFromTable\" }\n",
    "        }\n",
    "    }'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciar Crawler\n",
    "glue_client.start_crawler(Name='vendas-crawler')\n",
    "\n",
    "# Verificar status\n",
    "response = glue_client.get_crawler(Name='vendas-crawler')\n",
    "print(f\"Status: {response['Crawler']['State']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AWS Glue ETL Job (Script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este é um exemplo de script Glue ETL\n",
    "# Salve como .py e faça upload para S3\n",
    "\n",
    "glue_etl_script = '''\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "# Argumentos\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'input_path', 'output_path'])\n",
    "\n",
    "# Contexto\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# ============================================================================\n",
    "# LEITURA - Glue Catalog\n",
    "# ============================================================================\n",
    "datasource = glueContext.create_dynamic_frame.from_catalog(\n",
    "    database=\"meu_database\",\n",
    "    table_name=\"vendas\",\n",
    "    transformation_ctx=\"datasource\"\n",
    ")\n",
    "\n",
    "# Ou de S3 diretamente\n",
    "# datasource = glueContext.create_dynamic_frame.from_options(\n",
    "#     connection_type=\"s3\",\n",
    "#     connection_options={\"paths\": [args['input_path']]},\n",
    "#     format=\"parquet\"\n",
    "# )\n",
    "\n",
    "# ============================================================================\n",
    "# TRANSFORMAÇÕES\n",
    "# ============================================================================\n",
    "\n",
    "# Converter para DataFrame Spark\n",
    "df = datasource.toDF()\n",
    "\n",
    "# Transformações PySpark normais\n",
    "from pyspark.sql.functions import col, year, month\n",
    "\n",
    "df_transformed = df \\\n",
    "    .filter(col(\"valor\") > 0) \\\n",
    "    .withColumn(\"valor_total\", col(\"valor\") * col(\"quantidade\")) \\\n",
    "    .withColumn(\"ano\", year(\"data_venda\")) \\\n",
    "    .withColumn(\"mes\", month(\"data_venda\"))\n",
    "\n",
    "# Voltar para DynamicFrame\n",
    "dynamic_frame = DynamicFrame.fromDF(df_transformed, glueContext, \"dynamic_frame\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRANSFORMAÇÕES GLUE ESPECÍFICAS\n",
    "# ============================================================================\n",
    "\n",
    "# Apply Mapping - renomear e mudar tipos\n",
    "mapped = ApplyMapping.apply(\n",
    "    frame=dynamic_frame,\n",
    "    mappings=[\n",
    "        (\"id\", \"long\", \"id\", \"long\"),\n",
    "        (\"produto\", \"string\", \"nome_produto\", \"string\"),\n",
    "        (\"valor_total\", \"double\", \"valor_total\", \"double\"),\n",
    "        (\"ano\", \"int\", \"ano\", \"int\"),\n",
    "        (\"mes\", \"int\", \"mes\", \"int\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Drop Null Fields\n",
    "cleaned = DropNullFields.apply(frame=mapped)\n",
    "\n",
    "# ============================================================================\n",
    "# ESCRITA\n",
    "# ============================================================================\n",
    "\n",
    "# Escrever no Glue Catalog\n",
    "glueContext.write_dynamic_frame.from_catalog(\n",
    "    frame=cleaned,\n",
    "    database=\"meu_database\",\n",
    "    table_name=\"vendas_processed\",\n",
    "    transformation_ctx=\"write\"\n",
    ")\n",
    "\n",
    "# Ou escrever em S3\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "    frame=cleaned,\n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\n",
    "        \"path\": args['output_path'],\n",
    "        \"partitionKeys\": [\"ano\", \"mes\"]\n",
    "    },\n",
    "    format=\"parquet\",\n",
    "    format_options={\"compression\": \"snappy\"}\n",
    ")\n",
    "\n",
    "job.commit()\n",
    "'''\n",
    "\n",
    "print(glue_etl_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar Glue Job\n",
    "glue_client.create_job(\n",
    "    Name='vendas-etl-job',\n",
    "    Role='arn:aws:iam::123456789:role/GlueServiceRole',\n",
    "    Command={\n",
    "        'Name': 'glueetl',\n",
    "        'ScriptLocation': 's3://my-bucket/scripts/vendas_etl.py',\n",
    "        'PythonVersion': '3'\n",
    "    },\n",
    "    DefaultArguments={\n",
    "        '--job-language': 'python',\n",
    "        '--job-bookmark-option': 'job-bookmark-enable',\n",
    "        '--enable-metrics': '',\n",
    "        '--enable-continuous-cloudwatch-log': 'true',\n",
    "        '--input_path': 's3://my-bucket/raw/',\n",
    "        '--output_path': 's3://my-bucket/processed/'\n",
    "    },\n",
    "    MaxRetries=1,\n",
    "    GlueVersion='4.0',\n",
    "    NumberOfWorkers=2,\n",
    "    WorkerType='G.1X'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar job\n",
    "response = glue_client.start_job_run(\n",
    "    JobName='vendas-etl-job',\n",
    "    Arguments={\n",
    "        '--input_path': 's3://my-bucket/raw/2024/',\n",
    "        '--output_path': 's3://my-bucket/processed/2024/'\n",
    "    }\n",
    ")\n",
    "print(f\"Job Run ID: {response['JobRunId']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AWS Athena com Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "athena_client = boto3.client('athena', region_name='us-east-1')\n",
    "\n",
    "def run_athena_query(query, database, output_location):\n",
    "    \"\"\"Executa query no Athena e aguarda resultado\"\"\"\n",
    "    \n",
    "    # Iniciar query\n",
    "    response = athena_client.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={'Database': database},\n",
    "        ResultConfiguration={'OutputLocation': output_location}\n",
    "    )\n",
    "    \n",
    "    query_execution_id = response['QueryExecutionId']\n",
    "    print(f\"Query ID: {query_execution_id}\")\n",
    "    \n",
    "    # Aguardar conclusão\n",
    "    while True:\n",
    "        result = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "        state = result['QueryExecution']['Status']['State']\n",
    "        \n",
    "        if state == 'SUCCEEDED':\n",
    "            print(\"Query concluída!\")\n",
    "            break\n",
    "        elif state in ['FAILED', 'CANCELLED']:\n",
    "            reason = result['QueryExecution']['Status'].get('StateChangeReason', 'Unknown')\n",
    "            raise Exception(f\"Query {state}: {reason}\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Obter resultados\n",
    "    results = athena_client.get_query_results(QueryExecutionId=query_execution_id)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar query\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        produto,\n",
    "        SUM(valor * quantidade) as total_vendas,\n",
    "        COUNT(*) as qtd_vendas\n",
    "    FROM vendas\n",
    "    WHERE ano = 2024\n",
    "    GROUP BY produto\n",
    "    ORDER BY total_vendas DESC\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "results = run_athena_query(\n",
    "    query=query,\n",
    "    database='meu_database',\n",
    "    output_location='s3://my-bucket/athena-results/'\n",
    ")\n",
    "\n",
    "# Processar resultados\n",
    "for row in results['ResultSet']['Rows'][1:]:  # Skip header\n",
    "    values = [col.get('VarCharValue', '') for col in row['Data']]\n",
    "    print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando PyAthena (mais pythonic)\n",
    "# pip install pyathena\n",
    "\n",
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "\n",
    "conn = connect(\n",
    "    s3_staging_dir='s3://my-bucket/athena-results/',\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "# Query direta para pandas\n",
    "df = pd.read_sql(\"\"\"\n",
    "    SELECT * FROM meu_database.vendas\n",
    "    WHERE ano = 2024\n",
    "    LIMIT 100\n",
    "\"\"\", conn)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Spark + Glue Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark configurado para usar Glue Catalog\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkGlueCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"org.apache.spark:spark-hive_2.12:3.5.0\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.hive.metastore.client.factory.class\",\n",
    "            \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar databases do Glue\n",
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "\n",
    "# Usar database\n",
    "spark.sql(\"USE meu_database\")\n",
    "\n",
    "# Listar tabelas\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler tabela do Glue Catalog\n",
    "df = spark.table(\"meu_database.vendas\")\n",
    "df.show()\n",
    "\n",
    "# Ou via SQL\n",
    "df = spark.sql(\"\"\"\n",
    "    SELECT * FROM meu_database.vendas\n",
    "    WHERE ano = 2024 AND mes = 1\n",
    "\"\"\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escrever tabela no Glue Catalog\n",
    "df_processed = df.filter(df.valor > 100)\n",
    "\n",
    "df_processed.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .partitionBy(\"ano\", \"mes\") \\\n",
    "    .saveAsTable(\"meu_database.vendas_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualizar partições (MSCK REPAIR)\n",
    "spark.sql(\"MSCK REPAIR TABLE meu_database.vendas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Queries SQL Úteis no Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_athena = '''\n",
    "-- ============================================================================\n",
    "-- CRIAR TABELA EXTERNA\n",
    "-- ============================================================================\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS vendas (\n",
    "    id BIGINT,\n",
    "    produto STRING,\n",
    "    valor DOUBLE,\n",
    "    quantidade INT\n",
    ")\n",
    "PARTITIONED BY (ano INT, mes INT)\n",
    "STORED AS PARQUET\n",
    "LOCATION 's3://my-bucket/data/vendas/'\n",
    "TBLPROPERTIES ('parquet.compression'='SNAPPY');\n",
    "\n",
    "-- ============================================================================\n",
    "-- DESCOBRIR PARTIÇÕES\n",
    "-- ============================================================================\n",
    "MSCK REPAIR TABLE vendas;\n",
    "\n",
    "-- Ou adicionar manualmente\n",
    "ALTER TABLE vendas ADD PARTITION (ano=2024, mes=1)\n",
    "LOCATION 's3://my-bucket/data/vendas/ano=2024/mes=1/';\n",
    "\n",
    "-- ============================================================================\n",
    "-- CTAS - Create Table As Select\n",
    "-- ============================================================================\n",
    "CREATE TABLE vendas_2024\n",
    "WITH (\n",
    "    format = 'PARQUET',\n",
    "    parquet_compression = 'SNAPPY',\n",
    "    external_location = 's3://my-bucket/processed/vendas_2024/'\n",
    ") AS\n",
    "SELECT * FROM vendas WHERE ano = 2024;\n",
    "\n",
    "-- ============================================================================\n",
    "-- INSERT INTO\n",
    "-- ============================================================================\n",
    "INSERT INTO vendas_processed\n",
    "SELECT \n",
    "    id,\n",
    "    produto,\n",
    "    valor * quantidade as total,\n",
    "    ano,\n",
    "    mes\n",
    "FROM vendas\n",
    "WHERE ano = 2024;\n",
    "\n",
    "-- ============================================================================\n",
    "-- OPTIMIZE (Iceberg)\n",
    "-- ============================================================================\n",
    "-- OPTIMIZE vendas REWRITE DATA USING BIN_PACK;\n",
    "-- VACUUM vendas;\n",
    "\n",
    "-- ============================================================================\n",
    "-- QUERIES DE ANÁLISE\n",
    "-- ============================================================================\n",
    "-- Top produtos\n",
    "SELECT \n",
    "    produto,\n",
    "    SUM(valor * quantidade) as total_vendas\n",
    "FROM vendas\n",
    "GROUP BY produto\n",
    "ORDER BY total_vendas DESC\n",
    "LIMIT 10;\n",
    "\n",
    "-- Vendas por mês\n",
    "SELECT \n",
    "    ano,\n",
    "    mes,\n",
    "    SUM(valor * quantidade) as total\n",
    "FROM vendas\n",
    "GROUP BY ano, mes\n",
    "ORDER BY ano, mes;\n",
    "\n",
    "-- Window function\n",
    "SELECT \n",
    "    produto,\n",
    "    mes,\n",
    "    SUM(valor) as total_mes,\n",
    "    SUM(SUM(valor)) OVER (PARTITION BY produto ORDER BY mes) as acumulado\n",
    "FROM vendas\n",
    "GROUP BY produto, mes;\n",
    "'''\n",
    "\n",
    "print(queries_athena)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
