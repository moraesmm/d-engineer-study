{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark - Fundamentos\n",
    "\n",
    "Este notebook cobre os conceitos básicos de PySpark:\n",
    "- SparkSession e configurações\n",
    "- DataFrames e transformações\n",
    "- Actions vs Transformations\n",
    "- Window Functions\n",
    "- Joins e Agregações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Criando SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Forma básica\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark_Estudo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"App Name: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession com configurações avançadas\n",
    "spark_configured = SparkSession.builder \\\n",
    "    .appName(\"SparkConfigurado\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Criando DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Dados de exemplo\n",
    "data = [\n",
    "    (1, \"João\", \"Vendas\", 5000.0, \"2023-01-15\"),\n",
    "    (2, \"Maria\", \"TI\", 7500.0, \"2022-06-20\"),\n",
    "    (3, \"Pedro\", \"Vendas\", 4500.0, \"2023-03-10\"),\n",
    "    (4, \"Ana\", \"RH\", 6000.0, \"2021-11-05\"),\n",
    "    (5, \"Carlos\", \"TI\", 8000.0, \"2020-08-22\"),\n",
    "    (6, \"Julia\", \"TI\", 6500.0, \"2023-07-01\"),\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"nome\", \"departamento\", \"salario\", \"data_contratacao\"]\n",
    "\n",
    "# Criando DataFrame simples\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Com schema explícito (recomendado para produção)\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"nome\", StringType(), True),\n",
    "    StructField(\"departamento\", StringType(), True),\n",
    "    StructField(\"salario\", DoubleType(), True),\n",
    "    StructField(\"data_contratacao\", StringType(), True),\n",
    "])\n",
    "\n",
    "df_schema = spark.createDataFrame(data, schema)\n",
    "df_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo de arquivos (exemplos de sintaxe)\n",
    "# CSV\n",
    "# df_csv = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# JSON\n",
    "# df_json = spark.read.json(\"path/to/file.json\")\n",
    "\n",
    "# Parquet\n",
    "# df_parquet = spark.read.parquet(\"path/to/file.parquet\")\n",
    "\n",
    "# Com opções\n",
    "# df_csv = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .option(\"inferSchema\", \"true\") \\\n",
    "#     .option(\"delimiter\", \";\") \\\n",
    "#     .csv(\"path/to/file.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformações Básicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, when, upper, current_date\n",
    "\n",
    "# SELECT - selecionar colunas\n",
    "df.select(\"nome\", \"salario\").show()\n",
    "df.select(col(\"nome\"), col(\"salario\") * 1.1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER / WHERE - filtrar linhas\n",
    "df.filter(col(\"salario\") > 5000).show()\n",
    "df.where((col(\"departamento\") == \"TI\") & (col(\"salario\") >= 7000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHCOLUMN - adicionar/modificar colunas\n",
    "df_transformed = df \\\n",
    "    .withColumn(\"salario_anual\", col(\"salario\") * 12) \\\n",
    "    .withColumn(\"bonus\", col(\"salario\") * 0.1) \\\n",
    "    .withColumn(\"nome_upper\", upper(col(\"nome\"))) \\\n",
    "    .withColumn(\"data_atual\", current_date())\n",
    "\n",
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHEN/OTHERWISE - condicionais\n",
    "df_categoria = df.withColumn(\n",
    "    \"categoria_salario\",\n",
    "    when(col(\"salario\") < 5000, \"Junior\")\n",
    "    .when(col(\"salario\") < 7000, \"Pleno\")\n",
    "    .otherwise(\"Senior\")\n",
    ")\n",
    "df_categoria.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHCOLUMNRENAMED - renomear colunas\n",
    "df.withColumnRenamed(\"nome\", \"funcionario\").show()\n",
    "\n",
    "# DROP - remover colunas\n",
    "df.drop(\"data_contratacao\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Agregações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, avg, count, min, max, countDistinct\n",
    "\n",
    "# GroupBy com agregações\n",
    "df.groupBy(\"departamento\").agg(\n",
    "    count(\"*\").alias(\"total_funcionarios\"),\n",
    "    sum(\"salario\").alias(\"total_salarios\"),\n",
    "    avg(\"salario\").alias(\"media_salario\"),\n",
    "    min(\"salario\").alias(\"menor_salario\"),\n",
    "    max(\"salario\").alias(\"maior_salario\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Múltiplas agregações\n",
    "df.groupBy(\"departamento\").agg(\n",
    "    countDistinct(\"nome\").alias(\"funcionarios_unicos\"),\n",
    "    (sum(\"salario\") / count(\"*\")).alias(\"media_manual\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank, lead, lag\n",
    "\n",
    "# Definindo window\n",
    "window_dept = Window.partitionBy(\"departamento\").orderBy(col(\"salario\").desc())\n",
    "\n",
    "df_window = df \\\n",
    "    .withColumn(\"rank_salario\", rank().over(window_dept)) \\\n",
    "    .withColumn(\"dense_rank_salario\", dense_rank().over(window_dept)) \\\n",
    "    .withColumn(\"row_num\", row_number().over(window_dept))\n",
    "\n",
    "df_window.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lead e Lag - próximo e anterior\n",
    "window_ordered = Window.orderBy(\"salario\")\n",
    "\n",
    "df.withColumn(\"proximo_salario\", lead(\"salario\", 1).over(window_ordered)) \\\n",
    "    .withColumn(\"salario_anterior\", lag(\"salario\", 1).over(window_ordered)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running total (soma acumulada)\n",
    "window_running = Window.partitionBy(\"departamento\") \\\n",
    "    .orderBy(\"id\") \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "df.withColumn(\"salario_acumulado\", sum(\"salario\").over(window_running)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Criando tabela de departamentos\n",
    "departamentos = spark.createDataFrame([\n",
    "    (\"Vendas\", \"São Paulo\"),\n",
    "    (\"TI\", \"Rio de Janeiro\"),\n",
    "    (\"RH\", \"Belo Horizonte\"),\n",
    "], [\"departamento\", \"cidade\"])\n",
    "\n",
    "departamentos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner Join\n",
    "df.join(departamentos, \"departamento\", \"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left Join\n",
    "df.join(departamentos, \"departamento\", \"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast Join (otimização para tabelas pequenas)\n",
    "df.join(broadcast(departamentos), \"departamento\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Actions (executam o DAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions comuns\n",
    "print(\"Count:\", df.count())\n",
    "print(\"First:\", df.first())\n",
    "print(\"Take 3:\", df.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estatísticas\n",
    "df.describe(\"salario\").show()\n",
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Repartition e Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Partições originais:\", df.rdd.getNumPartitions())\n",
    "\n",
    "# Repartition - aumenta ou diminui (shuffle completo)\n",
    "df_repartitioned = df.repartition(4)\n",
    "print(\"Após repartition(4):\", df_repartitioned.rdd.getNumPartitions())\n",
    "\n",
    "# Coalesce - apenas diminui (sem shuffle)\n",
    "df_coalesced = df_repartitioned.coalesce(2)\n",
    "print(\"Após coalesce(2):\", df_coalesced.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition por coluna - dados da mesma chave na mesma partição\n",
    "df_by_dept = df.repartition(\"departamento\")\n",
    "print(\"Repartition por departamento:\", df_by_dept.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cache e Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "# Cache - armazena em memória\n",
    "df.cache()  # Equivalente a persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "# Persist com opções\n",
    "# StorageLevel.MEMORY_ONLY - só memória\n",
    "# StorageLevel.MEMORY_AND_DISK - memória com spillover para disco\n",
    "# StorageLevel.DISK_ONLY - só disco\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Remover do cache\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Explain - Plano de Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver plano de execução\n",
    "df.filter(col(\"salario\") > 5000) \\\n",
    "    .groupBy(\"departamento\") \\\n",
    "    .count() \\\n",
    "    .explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizando a sessão\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
