{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particionamento e Compressão no Spark\n",
    "\n",
    "Este notebook cobre:\n",
    "- Repartition vs Coalesce\n",
    "- PartitionBy ao escrever\n",
    "- Formatos de arquivo (Parquet, ORC, JSON, CSV)\n",
    "- Codecs de compressão\n",
    "- Estratégias de particionamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, rand\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Partitioning_Compression\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Criando Dataset de Exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_data(n_records=10000):\n",
    "    departamentos = [\"Vendas\", \"TI\", \"RH\", \"Financeiro\", \"Marketing\"]\n",
    "    cidades = [\"São Paulo\", \"Rio de Janeiro\", \"Belo Horizonte\", \"Porto Alegre\"]\n",
    "    \n",
    "    data = []\n",
    "    base_date = datetime(2022, 1, 1)\n",
    "    \n",
    "    for i in range(n_records):\n",
    "        data.append((\n",
    "            i,\n",
    "            f\"Funcionario_{i}\",\n",
    "            random.choice(departamentos),\n",
    "            random.choice(cidades),\n",
    "            random.uniform(3000, 15000),\n",
    "            (base_date + timedelta(days=random.randint(0, 730))).strftime(\"%Y-%m-%d\")\n",
    "        ))\n",
    "    return data\n",
    "\n",
    "data = generate_sample_data(10000)\n",
    "df = spark.createDataFrame(\n",
    "    data,\n",
    "    [\"id\", \"nome\", \"departamento\", \"cidade\", \"salario\", \"data_registro\"]\n",
    ")\n",
    "\n",
    "# Convertendo e adicionando colunas de data\n",
    "df = df.withColumn(\"data_registro\", col(\"data_registro\").cast(\"date\")) \\\n",
    "       .withColumn(\"ano\", year(\"data_registro\")) \\\n",
    "       .withColumn(\"mes\", month(\"data_registro\"))\n",
    "\n",
    "print(f\"Total de registros: {df.count()}\")\n",
    "print(f\"Partições no DataFrame: {df.rdd.getNumPartitions()}\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Repartition vs Coalesce\n",
    "\n",
    "### Diferenças:\n",
    "- **Repartition**: Redistribui dados (shuffle completo). Pode aumentar ou diminuir partições.\n",
    "- **Coalesce**: Apenas diminui partições. Sem shuffle (mais eficiente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Partições originais: {df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# REPARTITION - redistribui uniformemente\n",
    "df_repartitioned = df.repartition(10)\n",
    "print(f\"Após repartition(10): {df_repartitioned.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Repartition por coluna - mesma chave na mesma partição\n",
    "df_by_dept = df.repartition(\"departamento\")\n",
    "print(f\"Repartition por departamento: {df_by_dept.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Repartition por coluna com número específico\n",
    "df_by_dept_n = df.repartition(5, \"departamento\")\n",
    "print(f\"Repartition(5, 'departamento'): {df_by_dept_n.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COALESCE - reduz sem shuffle\n",
    "df_coalesced = df.coalesce(2)\n",
    "print(f\"Após coalesce(2): {df_coalesced.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Coalesce não pode aumentar partições!\n",
    "df_coalesced_up = df.coalesce(100)\n",
    "print(f\"Coalesce(100) - não aumenta: {df_coalesced_up.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PartitionBy ao Escrever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"./data/output\"\n",
    "\n",
    "# Particionamento simples por uma coluna\n",
    "# Estrutura: /departamento=Vendas/data.parquet\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"departamento\") \\\n",
    "    .parquet(f\"{output_path}/por_departamento\")\n",
    "\n",
    "print(\"Escrito com partição por departamento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particionamento hierárquico (múltiplas colunas)\n",
    "# Estrutura: /ano=2023/mes=01/data.parquet\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"ano\", \"mes\") \\\n",
    "    .parquet(f\"{output_path}/por_data\")\n",
    "\n",
    "print(\"Escrito com partição hierárquica ano/mes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Partition Pruning\n",
    "\n",
    "Quando você filtra por colunas de partição, Spark lê apenas as partições necessárias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo dados particionados\n",
    "df_partitioned = spark.read.parquet(f\"{output_path}/por_data\")\n",
    "\n",
    "# Esta query só lê partições necessárias\n",
    "df_filtered = df_partitioned.filter((col(\"ano\") == 2023) & (col(\"mes\") == 1))\n",
    "\n",
    "# Veja \"PartitionFilters\" no plano\n",
    "df_filtered.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Formatos de Arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de exemplo menor para comparação\n",
    "df_small = df.limit(1000)\n",
    "\n",
    "# PARQUET - Colunar (RECOMENDADO para analytics)\n",
    "df_small.write.mode(\"overwrite\").parquet(f\"{output_path}/formato_parquet\")\n",
    "\n",
    "# ORC - Colunar (otimizado para Hive)\n",
    "df_small.write.mode(\"overwrite\").orc(f\"{output_path}/formato_orc\")\n",
    "\n",
    "# JSON - Texto legível\n",
    "df_small.write.mode(\"overwrite\").json(f\"{output_path}/formato_json\")\n",
    "\n",
    "# CSV - Universal\n",
    "df_small.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_path}/formato_csv\")\n",
    "\n",
    "print(\"Arquivos escritos em diferentes formatos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparação de Formatos\n",
    "\n",
    "| Formato | Tipo | Compressão | Use Case |\n",
    "|---------|------|------------|----------|\n",
    "| Parquet | Colunar | Excelente | Analytics, Data Lake |\n",
    "| ORC | Colunar | Excelente | Hive, Analytics |\n",
    "| Avro | Linhas | Boa | Streaming, Kafka |\n",
    "| JSON | Texto | Ruim | APIs, Debug |\n",
    "| CSV | Texto | Ruim | Import/Export |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Codecs de Compressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNAPPY - Rápido, boa compressão (DEFAULT)\n",
    "df_small.write.mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .parquet(f\"{output_path}/parquet_snappy\")\n",
    "\n",
    "# GZIP - Maior compressão, mais lento\n",
    "df_small.write.mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .parquet(f\"{output_path}/parquet_gzip\")\n",
    "\n",
    "# ZSTD - Bom balanço velocidade/compressão\n",
    "df_small.write.mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"zstd\") \\\n",
    "    .parquet(f\"{output_path}/parquet_zstd\")\n",
    "\n",
    "# SEM compressão\n",
    "df_small.write.mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"none\") \\\n",
    "    .parquet(f\"{output_path}/parquet_none\")\n",
    "\n",
    "print(\"Comparação de compressão criada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codecs de Compressão\n",
    "\n",
    "| Codec | Velocidade | Compressão | Uso Recomendado |\n",
    "|-------|------------|------------|------------------|\n",
    "| snappy | Muito rápido | Média | Default, uso geral |\n",
    "| gzip | Lento | Alta | Armazenamento longo prazo |\n",
    "| lz4 | Muito rápido | Baixa | Streaming, tempo real |\n",
    "| zstd | Rápido | Alta | Melhor balanço |\n",
    "| none | N/A | Nenhuma | Debug, testes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Controlando Arquivos por Partição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problema comum: Muitos arquivos pequenos\n",
    "# Solução 1: Coalesce antes de escrever\n",
    "df.repartition(\"departamento\") \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"departamento\") \\\n",
    "    .parquet(f\"{output_path}/single_file_per_partition\")\n",
    "\n",
    "print(\"Um arquivo por partição\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solução 2: maxRecordsPerFile\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"maxRecordsPerFile\", 100000) \\\n",
    "    .partitionBy(\"departamento\") \\\n",
    "    .parquet(f\"{output_path}/controlled_size\")\n",
    "\n",
    "print(\"Tamanho de arquivo controlado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dynamic Partition Overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por padrão, overwrite substitui TODAS as partições\n",
    "# Com dynamic, só substitui partições com novos dados\n",
    "\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "# Agora só as partições presentes no DF serão substituídas\n",
    "novos_dados = df.filter(col(\"departamento\") == \"TI\")\n",
    "novos_dados.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"departamento\") \\\n",
    "    .parquet(f\"{output_path}/dynamic_overwrite\")\n",
    "\n",
    "print(\"Apenas partição TI foi sobrescrita\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Estratégias de Particionamento\n",
    "\n",
    "### Regras de Ouro:\n",
    "1. Partições não devem ser muito pequenas (< 128MB)\n",
    "2. Partições não devem ser muito grandes (> 1GB)\n",
    "3. Evite colunas com alta cardinalidade (ex: user_id)\n",
    "4. Partição ideal: 128MB - 1GB\n",
    "\n",
    "### Estratégias Comuns:\n",
    "- **Por DATA**: `partitionBy(\"ano\", \"mes\")` - time-series\n",
    "- **Por CATEGORIA**: `partitionBy(\"regiao\", \"tipo\")` - filtros frequentes\n",
    "- **Híbrido**: `partitionBy(\"ano\", \"mes\", \"regiao\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisando distribuição das partições\n",
    "def show_partition_sizes(df, name):\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"Número de partições: {df.rdd.getNumPartitions()}\")\n",
    "    partition_sizes = df.rdd.mapPartitions(lambda it: [sum(1 for _ in it)]).collect()\n",
    "    for i, size in enumerate(partition_sizes[:10]):  # Mostra apenas primeiras 10\n",
    "        print(f\"  Partição {i}: {size} registros\")\n",
    "\n",
    "show_partition_sizes(df, \"Original\")\n",
    "show_partition_sizes(df.repartition(4), \"Repartitioned(4)\")\n",
    "show_partition_sizes(df.repartition(\"departamento\"), \"Por departamento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modos disponíveis:\n",
    "# - overwrite: substitui se existir\n",
    "# - append: adiciona aos dados existentes\n",
    "# - ignore: não faz nada se existir\n",
    "# - error/errorifexists: erro se existir (default)\n",
    "\n",
    "df_small.write.mode(\"overwrite\").parquet(f\"{output_path}/test_mode\")\n",
    "df_small.write.mode(\"append\").parquet(f\"{output_path}/test_mode\")\n",
    "df_small.write.mode(\"ignore\").parquet(f\"{output_path}/test_mode\")\n",
    "\n",
    "# Verificando\n",
    "df_verify = spark.read.parquet(f\"{output_path}/test_mode\")\n",
    "print(f\"Total após overwrite + append: {df_verify.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar compressão padrão\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "spark.conf.set(\"spark.sql.orc.compression.codec\", \"snappy\")\n",
    "\n",
    "# Configurar partições de shuffle\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "\n",
    "# Ver configurações\n",
    "print(\"Parquet compression:\", spark.conf.get(\"spark.sql.parquet.compression.codec\"))\n",
    "print(\"Shuffle partitions:\", spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
